{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZjjdLnv7e17tIj8ViqVUj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gjeffroy/hyperparam_autotuning_keras/blob/main/NN_hyperparameter_tuning_witth_keras_tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07FeKGezfdU5",
        "outputId": "40b439bf-f407-4cc7-82e1-344c24095c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "! pip install keras\n",
        "! pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "\n",
        "# Define the model-building function\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=(28, 28)))  # Assuming input shape of (28, 28) for MNIST data\n",
        "\n",
        "    # Tune the number of hidden layers\n",
        "    for i in range(hp.Int('num_layers', 2, 20)):\n",
        "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
        "                                            min_value=32,\n",
        "                                            max_value=512,\n",
        "                                            step=32),\n",
        "                               activation='relu'))"
      ],
      "metadata": {
        "id": "VdNsETXlf6vd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "def load_and_prepare_mnist():\n",
        "    # Load the MNIST dataset\n",
        "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "    # Normalize pixel values to between 0 and 1\n",
        "    train_images = train_images / 255.0\n",
        "    test_images = test_images / 255.0\n",
        "\n",
        "    # Reshape images to the format (batch_size, height, width, channels)\n",
        "    train_images = train_images.reshape((-1, 28, 28, 1))\n",
        "    test_images = test_images.reshape((-1, 28, 28, 1))\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "    test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
        "\n",
        "    return (train_images, train_labels), (test_images, test_labels)\n",
        "\n",
        "# Load and prepare the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = load_and_prepare_mnist()\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Training images shape:\", train_images.shape)\n",
        "print(\"Training labels shape:\", train_labels.shape)\n",
        "print(\"Testing images shape:\", test_images.shape)\n",
        "print(\"Testing labels shape:\", test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMp4not0hFJs",
        "outputId": "ce994262-1c3c-4630-ead0-136d3015a199"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Training images shape: (60000, 28, 28, 1)\n",
            "Training labels shape: (60000, 10)\n",
            "Testing images shape: (10000, 28, 28, 1)\n",
            "Testing labels shape: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# Define the model-building function\n",
        "def build_model(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "    # Tune the number of hidden layers\n",
        "    for i in range(hp.Int('num_layers', 2, 20)):\n",
        "        model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
        "                                            min_value=32,\n",
        "                                            max_value=512,\n",
        "                                            step=32),\n",
        "                               activation='relu'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Tune learning rate\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "def load_and_prepare_mnist():\n",
        "    # Load the MNIST dataset\n",
        "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "    # Normalize pixel values to between 0 and 1\n",
        "    train_images = train_images / 255.0\n",
        "    test_images = test_images / 255.0\n",
        "\n",
        "    # Reshape images to the format (batch_size, height, width, channels)\n",
        "    train_images = train_images.reshape((-1, 28, 28, 1))\n",
        "    test_images = test_images.reshape((-1, 28, 28, 1))\n",
        "\n",
        "    return (train_images, train_labels), (test_images, test_labels)\n",
        "\n",
        "def main():\n",
        "    # Load and prepare the MNIST dataset\n",
        "    (train_images, train_labels), (test_images, test_labels) = load_and_prepare_mnist()\n",
        "\n",
        "    # Initialize tuner\n",
        "    tuner = RandomSearch(\n",
        "        build_model,\n",
        "        objective='val_accuracy',\n",
        "        max_trials=5,\n",
        "        executions_per_trial=3,\n",
        "        directory='my_dir',\n",
        "        project_name='mnist_tuning'\n",
        "    )\n",
        "\n",
        "    # Perform the hyperparameter search\n",
        "    tuner.search(train_images, train_labels, epochs=5, validation_split=0.1)\n",
        "\n",
        "    # Get the best model\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    # Train the best model on the full training dataset\n",
        "    best_model.fit(train_images, train_labels, epochs=10, validation_split=0.1)\n",
        "\n",
        "    # Evaluate the best model on the test dataset\n",
        "    loss, accuracy = best_model.evaluate(test_images, test_labels)\n",
        "    print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pgO2RT9hjz0",
        "outputId": "7a7536d3-6406-4d48-a059-585e9f486ba1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 02m 05s]\n",
            "val_accuracy: 0.9671666423479716\n",
            "\n",
            "Best val_accuracy So Far: 0.9756666620572408\n",
            "Total elapsed time: 00h 14m 33s\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0564 - accuracy: 0.9824 - val_loss: 0.0993 - val_accuracy: 0.9720\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0522 - accuracy: 0.9845 - val_loss: 0.1051 - val_accuracy: 0.9733\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0464 - accuracy: 0.9858 - val_loss: 0.0814 - val_accuracy: 0.9777\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0425 - accuracy: 0.9875 - val_loss: 0.1020 - val_accuracy: 0.9760\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0379 - accuracy: 0.9888 - val_loss: 0.0867 - val_accuracy: 0.9788\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0360 - accuracy: 0.9891 - val_loss: 0.0885 - val_accuracy: 0.9792\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0318 - accuracy: 0.9906 - val_loss: 0.0942 - val_accuracy: 0.9802\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0295 - accuracy: 0.9914 - val_loss: 0.1128 - val_accuracy: 0.9778\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0293 - accuracy: 0.9911 - val_loss: 0.1018 - val_accuracy: 0.9802\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0269 - accuracy: 0.9922 - val_loss: 0.1305 - val_accuracy: 0.9763\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1527 - accuracy: 0.9741\n",
            "Test accuracy: 0.9740999937057495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kerastuner import HyperParameters\n",
        "import os\n",
        "import json\n",
        "\n",
        "def summarize_tuner_attempts(directory):\n",
        "    tuner_summaries = []\n",
        "\n",
        "    # Iterate over each subdirectory in 'my_dir'\n",
        "    for subdir in os.listdir(directory):\n",
        "        subdir_path = os.path.join(directory, subdir)\n",
        "\n",
        "        # Check if it's a directory\n",
        "        if os.path.isdir(subdir_path):\n",
        "            # Check if it contains a 'trial.json' file\n",
        "            trial_file = os.path.join(subdir_path, 'trial.json')\n",
        "            if os.path.exists(trial_file):\n",
        "                # Load hyperparameters from 'trial.json'\n",
        "                with open(trial_file, 'r') as f:\n",
        "                    trial_data = json.load(f)\n",
        "                hp = HyperParameters.from_config(trial_data['hyperparameters'])\n",
        "\n",
        "                # Get the validation accuracy from 'trial.json'\n",
        "                val_accuracy = trial_data.get('score')\n",
        "\n",
        "                # Add hyperparameters and validation accuracy to the summaries list\n",
        "                tuner_summaries.append((hp, val_accuracy))\n",
        "\n",
        "    # Sort tuner summaries by the number of layers\n",
        "    tuner_summaries.sort(key=lambda x: x[0].values['num_layers'])\n",
        "\n",
        "    return tuner_summaries\n",
        "\n",
        "# Function to print hyperparameters as a table\n",
        "def print_hyperparameters_table(hp):\n",
        "    print(\"Number of Layers:\", hp.values['num_layers'])\n",
        "    print(\"Learning Rate:\", hp.values['learning_rate'])\n",
        "    print(\"\\nHyperparameters:\")\n",
        "    sorted_keys = sorted([key for key in hp.values.keys() if key.startswith('units')])\n",
        "    for key in sorted_keys:\n",
        "        print(f\"| {key}: {hp.values[key]} |\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Function to print summary table\n",
        "def print_summary_table(summary):\n",
        "    print(\"Summary:\")\n",
        "    print(\"| Attempt | Accuracy | Num Layers |\")\n",
        "    print(\"|---------|----------|------------|\")\n",
        "    for i, (hp, val_accuracy) in enumerate(summary, 1):\n",
        "        num_layers = hp.values['num_layers']\n",
        "        print(f\"| {i} | {val_accuracy} | {num_layers} |\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Example usage:\n",
        "summaries = summarize_tuner_attempts('my_dir/mnist_tuning')\n",
        "print_summary_table(summaries)\n",
        "for i, (hp, val_accuracy) in enumerate(summaries, 1):\n",
        "    print(f\"Attempt {i}:\")\n",
        "    print_hyperparameters_table(hp)\n",
        "    print(f\"Validation Accuracy: {val_accuracy}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSJp_YBJlmV2",
        "outputId": "24261f38-4c4e-4e4e-f349-5b26b28dd56c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "| Attempt | Accuracy | Num Layers |\n",
            "|---------|----------|------------|\n",
            "| 1 | 0.9671666423479716 | 2 |\n",
            "| 2 | 0.958388884862264 | 4 |\n",
            "| 3 | 0.9756666620572408 | 5 |\n",
            "| 4 | 0.9728333155314127 | 6 |\n",
            "| 5 | 0.9664999842643738 | 15 |\n",
            "\n",
            "\n",
            "Attempt 1:\n",
            "Number of Layers: 2\n",
            "Learning Rate: 0.01\n",
            "\n",
            "Hyperparameters:\n",
            "| units_0: 320 |\n",
            "| units_1: 256 |\n",
            "| units_10: 64 |\n",
            "| units_11: 288 |\n",
            "| units_12: 320 |\n",
            "| units_13: 384 |\n",
            "| units_14: 320 |\n",
            "| units_2: 192 |\n",
            "| units_3: 384 |\n",
            "| units_4: 288 |\n",
            "| units_5: 256 |\n",
            "| units_6: 32 |\n",
            "| units_7: 192 |\n",
            "| units_8: 32 |\n",
            "| units_9: 96 |\n",
            "\n",
            "\n",
            "Validation Accuracy: 0.9671666423479716\n",
            "\n",
            "Attempt 2:\n",
            "Number of Layers: 4\n",
            "Learning Rate: 0.01\n",
            "\n",
            "Hyperparameters:\n",
            "| units_0: 448 |\n",
            "| units_1: 224 |\n",
            "| units_10: 320 |\n",
            "| units_11: 512 |\n",
            "| units_12: 224 |\n",
            "| units_13: 160 |\n",
            "| units_14: 128 |\n",
            "| units_2: 352 |\n",
            "| units_3: 320 |\n",
            "| units_4: 288 |\n",
            "| units_5: 416 |\n",
            "| units_6: 256 |\n",
            "| units_7: 416 |\n",
            "| units_8: 64 |\n",
            "| units_9: 128 |\n",
            "\n",
            "\n",
            "Validation Accuracy: 0.958388884862264\n",
            "\n",
            "Attempt 3:\n",
            "Number of Layers: 5\n",
            "Learning Rate: 0.001\n",
            "\n",
            "Hyperparameters:\n",
            "| units_0: 96 |\n",
            "| units_1: 64 |\n",
            "| units_10: 160 |\n",
            "| units_11: 384 |\n",
            "| units_12: 224 |\n",
            "| units_13: 416 |\n",
            "| units_14: 192 |\n",
            "| units_2: 160 |\n",
            "| units_3: 352 |\n",
            "| units_4: 256 |\n",
            "| units_5: 448 |\n",
            "| units_6: 416 |\n",
            "| units_7: 256 |\n",
            "| units_8: 128 |\n",
            "| units_9: 320 |\n",
            "\n",
            "\n",
            "Validation Accuracy: 0.9756666620572408\n",
            "\n",
            "Attempt 4:\n",
            "Number of Layers: 6\n",
            "Learning Rate: 0.0001\n",
            "\n",
            "Hyperparameters:\n",
            "| units_0: 320 |\n",
            "| units_1: 384 |\n",
            "| units_2: 32 |\n",
            "| units_3: 32 |\n",
            "| units_4: 32 |\n",
            "| units_5: 32 |\n",
            "\n",
            "\n",
            "Validation Accuracy: 0.9728333155314127\n",
            "\n",
            "Attempt 5:\n",
            "Number of Layers: 15\n",
            "Learning Rate: 0.0001\n",
            "\n",
            "Hyperparameters:\n",
            "| units_0: 256 |\n",
            "| units_1: 192 |\n",
            "| units_10: 32 |\n",
            "| units_11: 32 |\n",
            "| units_12: 32 |\n",
            "| units_13: 32 |\n",
            "| units_14: 32 |\n",
            "| units_2: 160 |\n",
            "| units_3: 96 |\n",
            "| units_4: 320 |\n",
            "| units_5: 288 |\n",
            "| units_6: 32 |\n",
            "| units_7: 32 |\n",
            "| units_8: 32 |\n",
            "| units_9: 32 |\n",
            "\n",
            "\n",
            "Validation Accuracy: 0.9664999842643738\n",
            "\n"
          ]
        }
      ]
    }
  ]
}