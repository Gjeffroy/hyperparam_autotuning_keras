{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPaxsnvOmsdxs0AC/WlDfYG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gjeffroy/mnist-keras-tuner/blob/main/MNIST_CNN_hyperparameter_tuning_witth_keras_tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Libraries"
      ],
      "metadata": {
        "id": "BOBLmWb52Nsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install keras\n",
        "! pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMTgLTGlkBKd",
        "outputId": "8a124db6-9048-4315-b952-98312f5459d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries"
      ],
      "metadata": {
        "id": "KGx3aq9L2T3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from keras_tuner.tuners import RandomSearch, BayesianOptimization\n",
        "from keras_tuner import HyperParameters\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "6K77PuNR2i5F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  The different methods for NAS and hyperparameters tuning\n",
        "\n",
        "1. **Random Search**: Randomly samples hyperparameter configurations or neural network architectures within predefined ranges or search spaces, often serving as a baseline optimization technique.\n",
        "\n",
        "2. **Grid Search**: Exhaustively searches through a manually specified subset of the hyperparameter space, guaranteeing the optimal solution within the search space but can be computationally expensive for high-dimensional spaces.\n",
        "\n",
        "3. **Bayesian Optimization**: Utilizes probabilistic models, such as Gaussian processes, to intelligently select hyperparameters or architectures based on past evaluations, efficiently balancing exploration and exploitation.\n",
        "\n",
        "4. **Genetic Algorithms (GA)**: Inspired by natural selection, GA maintains a population of candidate solutions and evolves them over generations using selection, crossover, and mutation operations.\n",
        "\n",
        "5. **NeuroEvolution of Augmenting Topologies (NEAT)**: A genetic algorithm specifically designed for evolving neural network architectures, starting with simple architectures and evolving them over generations by adding or removing neurons and connections.\n",
        "\n",
        "6. **Gradient-Based Optimization**: Utilizes gradient information to optimize neural network architectures or hyperparameters, often using gradient descent variants like Adam and RMSProp.\n",
        "\n",
        "7. **Reinforcement Learning (RL)**: Formulates the architecture search problem as a reinforcement learning task, where an agent learns to sequentially select architectural decisions based on rewards obtained from evaluating candidate architectures.\n",
        "\n",
        "8. **Model-Based Optimization**: Constructs surrogate models of the objective function, such as Bayesian neural networks or Gaussian processes, to guide the search for optimal architectures or hyperparameters.\n",
        "\n",
        "9. **Evolutionary Strategies**: A family of optimization algorithms inspired by biological evolution, maintaining a population of candidate solutions and iteratively evolving them using mutation and recombination operators.\n",
        "\n",
        "10. **Meta-Learning**: Utilizes meta-learning techniques to learn optimization algorithms that adaptively search for optimal architectures or hyperparameters across different tasks or datasets.\n"
      ],
      "metadata": {
        "id": "Ch0DEx782_6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions and helpers"
      ],
      "metadata": {
        "id": "uDlCY6cD2ZY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading and preparing data"
      ],
      "metadata": {
        "id": "fE1OSKUy3b6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset\n",
        "def load_and_prepare_mnist():\n",
        "    # Load the MNIST dataset\n",
        "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "    # Normalize pixel values to between 0 and 1\n",
        "    train_images = train_images / 255.0\n",
        "    test_images = test_images / 255.0\n",
        "\n",
        "    # Reshape images to the format (batch_size, height, width, channels)\n",
        "    train_images = train_images.reshape((-1, 28, 28, 1))\n",
        "    test_images = test_images.reshape((-1, 28, 28, 1))\n",
        "\n",
        "    return (train_images, train_labels), (test_images, test_labels)"
      ],
      "metadata": {
        "id": "4BBWEjWz2n9c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building the model"
      ],
      "metadata": {
        "id": "Ynmk_rHd3fnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model-building function\n",
        "def build_model(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Tune the number of convolutional layers\n",
        "    model.add(tf.keras.layers.Conv2D(filters=hp.Int('conv_1_filters', min_value=32, max_value=256, step=32),\n",
        "                                     kernel_size=3,\n",
        "                                     activation='relu',\n",
        "                                     input_shape=(28, 28, 1)))\n",
        "\n",
        "    for i in range(hp.Int('num_conv_layers', 1, 3)):\n",
        "        model.add(tf.keras.layers.Conv2D(filters=hp.Int(f'conv_{i+2}_filters', min_value=32, max_value=256, step=32),\n",
        "                                         kernel_size=3,\n",
        "                                         activation='relu'))\n",
        "        model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "    # Tune the number of dense layers\n",
        "    for i in range(hp.Int('num_dense_layers', 1, 3)):\n",
        "        model.add(tf.keras.layers.Dense(units=hp.Int(f'dense_{i}_units', min_value=32, max_value=512, step=32),\n",
        "                                         activation='relu'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Tune learning rate\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "BzzMiKph2sFL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Printing out the results"
      ],
      "metadata": {
        "id": "E4YzhD933kD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_tuner_attempts(directory):\n",
        "    tuner_summaries = []\n",
        "\n",
        "    # Iterate over each subdirectory in 'my_dir'\n",
        "    for subdir in os.listdir(directory):\n",
        "        subdir_path = os.path.join(directory, subdir)\n",
        "\n",
        "        # Check if it's a directory\n",
        "        if os.path.isdir(subdir_path):\n",
        "            # Check if it contains a 'trial.json' file\n",
        "            trial_file = os.path.join(subdir_path, 'trial.json')\n",
        "            if os.path.exists(trial_file):\n",
        "                # Load hyperparameters from 'trial.json'\n",
        "                with open(trial_file, 'r') as f:\n",
        "                    trial_data = json.load(f)\n",
        "                hp = HyperParameters.from_config(trial_data['hyperparameters'])\n",
        "\n",
        "                # Get the validation accuracy from 'trial.json'\n",
        "                val_accuracy = trial_data.get('score')\n",
        "\n",
        "                # Add hyperparameters and validation accuracy to the summaries list\n",
        "                tuner_summaries.append((hp, val_accuracy))\n",
        "\n",
        "    # Sort tuner summaries by the number of convolutional layers\n",
        "    tuner_summaries.sort(key=lambda x: x[0].values['num_conv_layers'])\n",
        "\n",
        "    return tuner_summaries\n",
        "\n",
        "# Function to print CNN hyperparameters as a table\n",
        "def print_hyperparameters_table(hp):\n",
        "    print(\"Number of Convolutional Layers:\", hp.values['num_conv_layers'])\n",
        "    print(\"Number of Dense Layers:\", hp.values['num_dense_layers'])\n",
        "    print(\"Learning Rate:\", hp.values['learning_rate'])\n",
        "    print(\"\\nCNN Hyperparameters:\")\n",
        "    sorted_conv_keys = sorted([key for key in hp.values.keys() if key.startswith('conv')])\n",
        "    for key in sorted_conv_keys:\n",
        "        print(f\"| {key}: {hp.values[key]} |\")\n",
        "    print(\"\\nDense Layer Hyperparameters:\")\n",
        "    sorted_dense_keys = sorted([key for key in hp.values.keys() if key.startswith('dense')])\n",
        "    for key in sorted_dense_keys:\n",
        "        print(f\"| {key}: {hp.values[key]} |\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Function to print summary table\n",
        "def print_summary_table(summary):\n",
        "    print(\"Summary:\")\n",
        "    print(\"| Attempt | Accuracy | Num Conv Layers | Num Dense Layers |\")\n",
        "    print(\"|---------|----------|-----------------|-------------------|\")\n",
        "    for i, (hp, val_accuracy) in enumerate(summary, 1):\n",
        "        num_conv_layers = hp.values['num_conv_layers']\n",
        "        num_dense_layers = hp.values['num_dense_layers']\n",
        "        print(f\"| {i} | {val_accuracy} | {num_conv_layers} | {num_dense_layers} |\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcLi5WwS3aVU",
        "outputId": "6d8b7619-3e62-4a43-8a0d-5292b6c1ad58"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d173f1c25101>:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner import HyperParameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Global parameters\n",
        "\n",
        "`OPTIM_OBJ`: This variable specifies the metric to optimize during the tuning process. It could be a string representing a metric such as accuracy or loss.\n",
        "\n",
        "`MAX_TRIAL_TUNER`: This variable determines the maximum number of different hyperparameter combinations that will be tested during the tuning process. Once this number is reached, the tuning process stops.\n",
        "\n",
        "`NUM_EXEC_PER_TRIAL`: This variable specifies the number of executions to run for each trial (i.e., each set of hyperparameters). This is useful for reducing the effect of randomness in the training process.\n",
        "\n",
        "`RESULT_DIR`: This variable specifies the directory where the tuning results will be saved.\n",
        "\n",
        "`NUM_EPOCH_SEARCH`: This variable specifies the number of epoch to run in each execution during the tuning process\n",
        "\n",
        "`NUM_EPOCH_VAL`: This variable specifies the number of epoch to run in each execution when validating the best set of hyperparameter found\n",
        "\n",
        "`VAL_SPLIT`: This variable specifies the size of the validation set as a ratio of the entire set\n"
      ],
      "metadata": {
        "id": "6CQHTe8s6MLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global parameter\n",
        "MAX_TRIAL_TUNER = 5\n",
        "NUM_EXEC_PER_TRIAL = 3\n",
        "NUM_EPOCH_SEARCH = 5\n",
        "NUM_EPOCH_VAL = 10\n",
        "VAL_SPLIT = 0.1\n",
        "OPTIM_OBJ = 'val_accuracy'\n",
        "RESULT_DIR = \"results\""
      ],
      "metadata": {
        "id": "oywAtVYZ3wyr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Search optimisation\n",
        "\n",
        "Random search is a fundamental technique in neural architecture search (NAS) and hyperparameter optimization. It operates by randomly sampling hyperparameter configurations or neural network architectures within predefined ranges or search spaces. Unlike more sophisticated methods like Bayesian optimization or evolutionary algorithms, random search does not leverage past evaluations to inform subsequent samples. Despite its simplicity, random search is remarkably effective, often outperforming more complex methods in practice due to its ability to explore a wide range of configurations efficiently. However, its effectiveness heavily depends on the size and structure of the search space. While random search may not guarantee finding the optimal solution, it serves as a strong baseline and is widely used in both NAS and hyperparameter optimization experiments.\n",
        "\n",
        "More on the RandomSearch : https://keras.io/api/keras_tuner/tuners/random/"
      ],
      "metadata": {
        "id": "A38XY6WX2wxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBp3chjcj3wK",
        "outputId": "65ae34b2-7dac-4772-deb5-bb0ea17771e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4 Complete [00h 03m 33s]\n",
            "val_accuracy: 0.39455554882685345\n",
            "\n",
            "Best val_accuracy So Far: 0.991611103216807\n",
            "Total elapsed time: 00h 15m 45s\n",
            "\n",
            "Search: Running Trial #5\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "96                |32                |conv_1_filters\n",
            "3                 |3                 |num_conv_layers\n",
            "192               |96                |conv_2_filters\n",
            "1                 |2                 |num_dense_layers\n",
            "416               |448               |dense_0_units\n",
            "0.01              |0.001             |learning_rate\n",
            "32                |192               |conv_3_filters\n",
            "192               |160               |conv_4_filters\n",
            "256               |32                |dense_1_units\n",
            "224               |None              |dense_2_units\n",
            "\n",
            "Epoch 1/5\n",
            "1688/1688 [==============================] - 16s 8ms/step - loss: 2.3039 - accuracy: 0.1090 - val_loss: 2.3045 - val_accuracy: 0.1050\n",
            "Epoch 2/5\n",
            "1688/1688 [==============================] - 13s 7ms/step - loss: 2.3026 - accuracy: 0.1109 - val_loss: 2.3032 - val_accuracy: 0.1050\n",
            "Epoch 3/5\n",
            "1688/1688 [==============================] - 13s 7ms/step - loss: 2.3023 - accuracy: 0.1097 - val_loss: 2.3029 - val_accuracy: 0.1050\n",
            "Epoch 4/5\n",
            "1422/1688 [========================>.....] - ETA: 1s - loss: 2.3024 - accuracy: 0.1113"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Load and prepare the MNIST dataset\n",
        "    (train_images, train_labels), (test_images, test_labels) = load_and_prepare_mnist()\n",
        "\n",
        "    # Initialize tuner\n",
        "    tuner = RandomSearch(\n",
        "        build_model,\n",
        "        objective=OPTIM_OBJ,\n",
        "        max_trials=MAX_TRIAL_TUNER,\n",
        "        executions_per_trial=NUM_EXEC_PER_TRIAL,\n",
        "        directory=RESULT_DIR,\n",
        "        project_name='mnist_tuning_random'\n",
        "    )\n",
        "\n",
        "    # Perform the hyperparameter search\n",
        "    tuner.search(train_images, train_labels, epochs=NUM_EPOCH_SEARCH, validation_split=VAL_SPLIT)\n",
        "\n",
        "    # Get the best model\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    # Print the best hyperparameters\n",
        "    print(\"\\nBest Hyperparameters:\")\n",
        "    print(best_model.get_config())\n",
        "\n",
        "    # Train the best model on the full training dataset\n",
        "    print(\"\\nTraining the best model...\")\n",
        "    best_model.fit(train_images, train_labels, epochs=NUM_EPOCH_VAL, validation_split=VAL_SPLIT)\n",
        "\n",
        "    # Evaluate the best model on the test dataset\n",
        "    print(\"\\nEvaluating the best model on the test dataset...\")\n",
        "    loss, accuracy = best_model.evaluate(test_images, test_labels)\n",
        "    print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the outcomes and summary\n",
        "summaries = summarize_tuner_attempts('results/mnist_tuning_random')\n",
        "print_summary_table(summaries)\n",
        "for i, (hp, val_accuracy) in enumerate(summaries, 1):\n",
        "    print(f\"Attempt {i}:\")\n",
        "    print_hyperparameters_table(hp)\n",
        "    print(f\"Validation Accuracy: {val_accuracy}\\n\")"
      ],
      "metadata": {
        "id": "4y1jXsnQpnv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimization\n",
        "Bayesian optimization is a powerful technique for optimizing expensive-to-evaluate black-box functions, commonly used in neural architecture search (NAS) and hyperparameter optimization. Unlike random search, Bayesian optimization intelligently selects the next set of hyperparameters or neural network architectures based on past evaluations, aiming to maximize the expected improvement in performance. It models the objective function using a surrogate probabilistic model, typically a Gaussian process, which provides insights into the function's behavior and uncertainty. By iteratively balancing exploration (sampling from uncertain regions) and exploitation (sampling around promising regions), Bayesian optimization efficiently converges to the optimal solution with fewer evaluations compared to random search. Its ability to incorporate prior knowledge and adaptively explore the search space makes it highly effective, especially in scenarios where the evaluation budget is limited."
      ],
      "metadata": {
        "id": "bIRhirBd8P0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load and prepare the MNIST dataset\n",
        "    (train_images, train_labels), (test_images, test_labels) = load_and_prepare_mnist()\n",
        "\n",
        "    # Initialize tuner\n",
        "    tuner = BayesianOptimization(\n",
        "        build_model,\n",
        "        objective=OPTIM_OBJ,\n",
        "        max_trials=MAX_TRIAL_TUNER,\n",
        "        directory=RESULT_DIR,\n",
        "        project_name='mnist_tuning_baye'\n",
        "    )\n",
        "\n",
        "    # Perform the hyperparameter search\n",
        "    tuner.search(train_images, train_labels, epochs=5, validation_split=VAL_SPLIT)\n",
        "\n",
        "    # Get the best model\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    # Print the best hyperparameters\n",
        "    print(\"\\nBest Hyperparameters:\")\n",
        "    print(best_model.get_config())\n",
        "\n",
        "    # Train the best model on the full training dataset\n",
        "    print(\"\\nTraining the best model...\")\n",
        "    best_model.fit(train_images, train_labels, epochs=10, validation_split=VAL_SPLIT)\n",
        "\n",
        "    # Evaluate the best model on the test dataset\n",
        "    print(\"\\nEvaluating the best model on the test dataset...\")\n",
        "    loss, accuracy = best_model.evaluate(test_images, test_labels)\n",
        "    print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "xndna-6d5HsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the outcomes and summary\n",
        "summaries = summarize_tuner_attempts('results/mnist_tuning_baye')\n",
        "print_summary_table(summaries)\n",
        "for i, (hp, val_accuracy) in enumerate(summaries, 1):\n",
        "    print(f\"Attempt {i}:\")\n",
        "    print_hyperparameters_table(hp)\n",
        "    print(f\"Validation Accuracy: {val_accuracy}\\n\")"
      ],
      "metadata": {
        "id": "6eiNE0Im5ys7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uie5cAhb8Moq"
      }
    }
  ]
}